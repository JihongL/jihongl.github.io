[{"content":"Microsoft Power Apps의 테스트를 진행 과정에 DB 연결 실패 오류가 계속 발생하여 진행이 어려웠다. Whitelist 기반 보안이어서 Power Apps가 사용하는 IP 대역을 확인하여 추가하니 동작하였음. 비슷한 경우가 생길 경우를 대비, 까먹기 전에 적어둔다.\nPower Apps IP Addresses 에러가 발생한 시점에 티켓을 발행했지만, 대응이 만족스럽지 못했다. 결국 직접 찾아보기 시작하여 Power Apps 에서 사용하는 IP 대역을 찾을 수 있었다. 현 조직은 Asia Region이기에 해당하는 대역을 모두 방화벽에서 허용해주기 시작했다.\n   Region Outbound IP     Asia1 13.75.113.224 52.187.147.27 52.175.23.169 52.187.68.19 13.75.36.64 - 13.75.36.79 104.214.164.0 - 104.214.164.31 13.67.8.240 - 13.67.8.255 13.67.15.32 - 13.67.15.63    네 번째 항목부터는 범위이기에 CIDR로 대응했다.\n   as-is to-be     13.75.36.64 - 13.75.36.79 104.214.164.0 - 104.214.164.31 13.67.8.240 - 13.67.8.255 13.67.15.32 - 13.67.15.63 13.75.36.64/28 104.214.164.0/27 13.67.8.240/27 13.67.15.32/27    각 IP를 방화벽에 추가한 후 다시 Power Apps 데이터 연결을 시도하니 성공했음.\n번외로 CIDR 계산이 번거로워 누가 계산기 만들어 둔 것 없을까 찾아보니 이미 있더라. 이 사이트2도 까먹지 않게 노트.\n  2021년 8월 기준\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n CIDR 계산기\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://thehong.xyz/post/powerapps/ip-address/","summary":"Microsoft Power Apps의 테스트를 진행 과정에 DB 연결 실패 오류가 계속 발생하여 진행이 어려웠다. Whitelist 기반 보안이어서 Power Apps가 사용하는 IP 대역을 확인하여 추가하니 동작하였음. 비슷한 경우가 생길 경우를 대비, 까먹기 전에 적어둔다.\nPower Apps IP Addresses 에러가 발생한 시점에 티켓을 발행했지만, 대응이 만족스럽지 못했다. 결국 직접 찾아보기 시작하여 Power Apps 에서 사용하는 IP 대역을 찾을 수 있었다. 현 조직은 Asia Region이기에 해당하는 대역을 모두 방화벽에서 허용해주기 시작했다.\n   Region Outbound IP     Asia1 13.","title":"Power Apps 방화벽 허용 설정"},{"content":"이전에 이슈 진행 과정을 jira workflows1에서 정의했다. 이 workflow 양식을 따르는 이슈는 특정 사용자의 승인이 완료되어야 다음 단계로 진행할 수 있는데\u0026hellip; 보통 승인권자들이 바쁘다 보니 이런저런 이유로 지연되는 경우가 빈번히 발생하였다. 매번 직접 연락드리기도 민망하여 jira automate로 지연되고 있다는 사실을 알려주는 규칙을 구성하였는데, 까먹기 전에 공유.\nAutomate로 매일 9시에 알림 메일 보내기 구상한 조건은 다음과 같다.\n 매일 아침 9시에 동작 승인 단계에서 3일 이상 지연된 이슈 대상 승인권자에게 승인이 필요함을 알려주는 메일 발송 메일 내용에는 요청자와 이슈 내용을 볼 수 있는 링크 등이 포함  매일 아침 9시에 동작 Automate 설정으로 이동, Create Rule을 선택하여 규칙 생성으로 이동하면 해당 규칙이 어떤 조건에서 실행할 것인지 트리거를 선택하도록 안내한다. 리스트 맨 밑, 혹은 검색창에 Scheduled를 입력하면 Scheduled 트리거를 찾을 수 있다. 해당 트리거를 선택.\n        Scheduled trigger를 선택    단순히 매일 보낼 것이라면 Basic에서 every 1 day를 선택하면 된다. 하지만, 이 경우 주말에도 메일이 발송될 것. 역지사지, 나도 주말에 업무 메일을 받기 싫으니 매너를 지키고 싶다. Advanced (Cron expression) 설정으로 이동하여 주중 오전 9시로 스케쥴 확정.\n        CRON으로 주중에만 보내도록 설정    주의할 것은 CRON이 UTC 기준으로 동작한다는 것이다. 한국기준 KST는 UTC보다 9시간이 빠르므로 오전 9시에 동작하길 원한다면 CRON은 0시로 설정해야 한다. CronMaker 같은 곳을 이용하면 더 쉽게 규칙을 설정할 수 있다.\n승인 단계에서 3일 이상 지연된 이슈 대상 메일을 보낼 대상은 승인이 3일 이상 지연된 이슈. Scheduled trigger에서 JQL을 통해 그 조건을 적용할 수 있다. 현재 적용하고자 하는 조건을 JQL로 나타내면 다음과 같다.\nstatus = \u0026quot;Waiting for Approval\u0026quot; and updatedDate \u0026lt;= -3d 해당 쿼리를 입력하고 Validate Query를 선택하자. JQL 조건에 맞는 이슈가 필터링 되기에 의도한 조건대로 동작하는지 확인할 수 있다.\n        JQL로 조건을 설정해준다    그리고 이어지는 조건2을 비활성화해준다. 해당 조건이 활성화되어 있다면 4일 이상 지연되는 경우 메일이 보내지지 않는다. 지난번 규칙이 실행된 후 변경된 이슈만 포함하는 조건이기 때문. 승인권자가 승인해 줄 때까지 매일 보낼 것이기에 필요 없는 옵션이다.\n승인권자에게 메일 보내기 Scheduled trigger를 저장하고 component를 추가하자. New Action에서 Send email을 찾아서 추가. 받는 이의 드롭다운 메뉴에서 Approvers를 선택하면 각 이슈의 승인권자가 수신인으로 등록된다.\n        Send mail 기능으로 승인권자에게 메일 발송    제목과 본문에서 모두 smart value를 지원하기에 이슈의 내용을 쉽게 포함할 수 있다. 주로 추가하고자 하는 내용은 이슈를 등록한 사용자명3, 이슈 요약4, 이슈 링크5. 그 밖에 필요한 내용을 추가하여 형식을 지정해준다.\n        smart value로 내용을 채운다    빠른 승인 부탁드립니다 모든 설정이 완료되었으면 저장 및 배포하여 프로젝트에 적용한다. 이제 매일 아침 9시에 승인권자에게 알림이 갈 것이다.\n        메일 예시    물론 메일을 매일 받고도 반응이 없는 분도 계심. 하지만 jira는 매일 동작할 것이다. 승인을 받을 때까지\u0026hellip; 모쪼록 빠른 승인 부탁드립니다.\n  Jira Service Management 구성기 - Workflows와 Approval\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Only include issues that have changed since the last time this rule executed\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n {{issue.reporter.displayName}}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n {{issue.summary}}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n {{issue.url}}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://thehong.xyz/post/jira/automate-mail/","summary":"이전에 이슈 진행 과정을 jira workflows1에서 정의했다. 이 workflow 양식을 따르는 이슈는 특정 사용자의 승인이 완료되어야 다음 단계로 진행할 수 있는데\u0026hellip; 보통 승인권자들이 바쁘다 보니 이런저런 이유로 지연되는 경우가 빈번히 발생하였다. 매번 직접 연락드리기도 민망하여 jira automate로 지연되고 있다는 사실을 알려주는 규칙을 구성하였는데, 까먹기 전에 공유.\nAutomate로 매일 9시에 알림 메일 보내기 구상한 조건은 다음과 같다.\n 매일 아침 9시에 동작 승인 단계에서 3일 이상 지연된 이슈 대상 승인권자에게 승인이 필요함을 알려주는 메일 발송 메일 내용에는 요청자와 이슈 내용을 볼 수 있는 링크 등이 포함  매일 아침 9시에 동작 Automate 설정으로 이동, Create Rule을 선택하여 규칙 생성으로 이동하면 해당 규칙이 어떤 조건에서 실행할 것인지 트리거를 선택하도록 안내한다.","title":"Jira Automate - 지연된 이슈에 알림 설정"},{"content":"Power BI 작업을 하면 대부분 분석계 데이터베이스를 소스로 데이터를 확보하는 경우가 대부분이다. 하지만, DB에 익숙하지 않은 조직원들이 Power BI를 활용할 때는 아무래도 Excel을 소스로 활용하게 되는 경우가 많다. 아직 DB에 쌓이지 않은 데이터를 활용해야 할 때도 마찬가지.\n당연히 Excel로 소스로 활용할 수 있어 당장 개인이 작업할 때는 문제가 없다. 이슈가 생기기 시작할 때는 Power BI Service에 게시한 이후. 개인 로컬 환경에 있는 Excel을 원본으로 하는 보고서를 그대로 게시하면 이후 새로 고침을 위해 Personal Gateway 구성이 필요하다.\n이름 그대로 개인이 활용하기 위한 Gateway이기에 활용이 제한적인데 설치 및 유지가 번거롭다. 그렇다고 매번 로컬에서 pbix 파일을 새로 고쳐서 게시하는 것도 상당히 비효율적인 일. 만약 조직에서 OneDrive For Buisiness(이하 OneDrive)나 Sharepoint를 사용한다면 해결할 수 있다. 우선 까먹기 전에 방법부터 공유.\nOneDrive에 Excel 파일 업로드 우선 OneDrive를 활용해보자. OneDrive에 Excel 파일을 업로드 하면 준비 완료다.\n        업로드 후 모습    다음으로 데스크톱 앱을 통해 해당 파일을 연다. 익숙한 UI가 보일 텐데, 좌측 상단의 파일을 누른 후, 차례로 정보 탭으로 이동. 파일의 이름이 보일 텐데, 그 밑에 보이는 경로 복사를 선택한다.\n        파일 \u0026gt; 정보 \u0026gt; 경로 복사 순서로 선택    Web 커넥터로 Excel을 불러오자 이어서 Power BI Desktop으로 이동, Get data 를 선택한다. 너무 종류가 많으니 검색창에 web을 입력하여 나타나는 Web 커넥터를 선택하고 Connect로 이어간다.\n        Web 커넥터를 선택하여 연결    이제 나타나는 주소창에 아까 복사한 엑셀 파일의 경로를 붙여넣는다. 그리고 확인을 누르기 전, 파일 확장자 뒤의 부분은 모두 지워준다.\n        xlsx 확장자 뒤의 내용은 삭제    그리고 OK를 선택하면 Excel 파일이 불러와 진다. 원하는 데이터를 선택하여 불러오면 완료!\n        Web 커넥터로 Excel 데이터 확인    게시 후 새로 고침을 위한 설정 마지막으로 할 일이 남았다. Power BI Service에서 데이터 원본에 대한 인증 작업을 완료해야 새로 고침을 활용할 수 있기 때문. 해당 보고서가 게시된 작업영역으로 이동하자. 게시된 보고서와 함께 데이터 세트가 함께 보일 텐데, 이 중 데이터 세트의 설정으로 이동한다.\n        데이터 세트 설정으로 이동    이동한 페이지에서 바로 에러 메시지가 맞이해 줄 것이다. 자격 증명이 잘못되었다는, 얼핏 듣기에 심각하게 느껴질 수 있는 내용이다. 이는 Power BI Service에서 OneDrive에 연결하기 위한 인증 정보가 없기에 나타나는 메시지다. Web 오른편에 보이는 자격 증명 편집을 선택하자.\n        험악한 메시지를 뒤로하고 자격 증명 편집을 선택    이어서 보이는 구성 페이지의 인증 방법의 드롭다운 버튼을 선택. 다양한 인증 수단이 표시되는데, 그 중 OAuth2를 선택하고 로그인한다. OneDrive에 업로드된 Excel 파일에 접근할 수 있는 계정으로 로그인하여야 한다.\n        OAuth2를 선택하여 인증    정상적으로 인증이 완료되면 처음에 나타나던 에러 메시지가 사라졌을 것이다. 데이터 세트 새로 고침 또한 가능. 엑셀 원본이 웹에 위치하기에 Personal Gateway 또한 필요가 없다.\n이를 통해 엑셀로 관리되는 데이터도 간편하게 새로 고침 설정을 할 수 있고, 최신 데이터가 표시되도록 유지할 수 있다. Power BI의 컨셉을 설명하거나 데이터베이스 활용이 쉽지 않은 다양한 환경에서 유용하게 적용할 수 있을 것이다.\n","permalink":"https://thehong.xyz/post/powerbi/source-from-web/","summary":"Power BI 작업을 하면 대부분 분석계 데이터베이스를 소스로 데이터를 확보하는 경우가 대부분이다. 하지만, DB에 익숙하지 않은 조직원들이 Power BI를 활용할 때는 아무래도 Excel을 소스로 활용하게 되는 경우가 많다. 아직 DB에 쌓이지 않은 데이터를 활용해야 할 때도 마찬가지.\n당연히 Excel로 소스로 활용할 수 있어 당장 개인이 작업할 때는 문제가 없다. 이슈가 생기기 시작할 때는 Power BI Service에 게시한 이후. 개인 로컬 환경에 있는 Excel을 원본으로 하는 보고서를 그대로 게시하면 이후 새로 고침을 위해 Personal Gateway 구성이 필요하다.","title":"Personal Gateway 없이 Power BI Service에서 Excel을 소스로 쓰기"},{"content":"Microsoft Power BI의 구축과 운영을 담당하며 겪었던 이벤트를 기록하려 한다. 그 첫 번째로 Power BI 테넌트를 홍콩에서 서울로 이전한 경험을 까먹기 전에 공유.\nPower BI 테넌트, 홍콩에 있다구? 현 조직의 Power BI를 구축한 후, Power BI Service에서 보고서를 조회할 때 뭔가 반응이 느리다는 느낌을 받아왔다. 하여 성능개선을 위해 다양한 방안을 시도하던 중, 홍콩 리전의 Power BI 테넌트와 연결된 것을 확인할 수 있었다.\n        테넌트 확인 절차    테넌트를 서울로 옮기면 물리적인 거리가 줄기에 통신 시간이 줄어들 뿐더러, 해외 데이터 센터에 데이터가 저장됨으로 발생할 수 있는 각종 문제도 예방할 수 있을 것으로 기대됐다.\n정녕 감당할 수 있으십니까 테넌트 수정을 위해 이리저리 메뉴를 뒤져봤지만 관련된 설정을 찾을 수 없었다. 불행히도 한 번 정해진 테넌트는 변경할 수 없다는 문서를 확인하게 되었음. 하지만 여기서 포기할 수 없지. Support Ticket을 발행하여 Microsoft에 직접 문의, 조직과 연결된 테넌트를 변경할 수 있다는 것을 알게 되었다.\n여기서 주의할 것, 기존 테넌트의 설정과 게시된 보고서 등 모든 데이터가 소멸된다. 다행히 막 Power BI를 도입한 시기였기에 상대적으로 영향도가 작다고 판단, 기존 홍콩 테넌트를 폐기하고 서울 테넌트를 새로이 연결하기로 결정했다.\nSupport Ticket은 현재 Power platfrom admin center에서 발행할 수 있으니 참고.\n테넌트 변경과 관련된 참고사항 Microsoft와 테넌트 이전 관련하여 확인한 내용은 아래와 같다.\n 기존 테넌트의 각종 설정과 게시된 보고서 등 모든 데이터는 소멸 테넌트를 새로이 연동하는 작업은 약 2~3시간 소요 현 시점으로부터 최소 2주 뒤 일정으로 진행할 수 있음  서로 다른 3개의 일정(희망하는 일자와 시간)을 전달    장막을 들추고 엿보니 MS와 협의한 일정은 일요일 저녁이었다. 해외 사무실에 자리한 엔지니어가 대응해준다는 듯. 시작부터 약 2시간이 소요된다고 한 작업, 궁금해서 한 30분 후에 슬쩍 천막을 들춰봤는데\u0026hellip;\n        촛불을 꺼뜨리는가?    벌써 마무리가 다 되어 서울 테넌트로 변경되어 있었다.\n확실히 위치가 홍콩에서 한국으로 바뀌니 같은 보고서도 좀 더 빠르게 조회가 되는 것이 느껴졌다.\n이 글을 읽는 분은 대부분 한국 사용자일텐데, 데이터 소멸에 대한 영향도가 크기 않다면 이전을 권한다.\n","permalink":"https://thehong.xyz/post/powerbi/change-tenant/","summary":"Microsoft Power BI의 구축과 운영을 담당하며 겪었던 이벤트를 기록하려 한다. 그 첫 번째로 Power BI 테넌트를 홍콩에서 서울로 이전한 경험을 까먹기 전에 공유.\nPower BI 테넌트, 홍콩에 있다구? 현 조직의 Power BI를 구축한 후, Power BI Service에서 보고서를 조회할 때 뭔가 반응이 느리다는 느낌을 받아왔다. 하여 성능개선을 위해 다양한 방안을 시도하던 중, 홍콩 리전의 Power BI 테넌트와 연결된 것을 확인할 수 있었다.\n        테넌트 확인 절차    테넌트를 서울로 옮기면 물리적인 거리가 줄기에 통신 시간이 줄어들 뿐더러, 해외 데이터 센터에 데이터가 저장됨으로 발생할 수 있는 각종 문제도 예방할 수 있을 것으로 기대됐다.","title":"Power BI 테넌트 서울 리전으로 변경"},{"content":"몸담은 팀의 업무 범위가 늘어나며 문의가 점점 많아지기 시작했다. 다른 팀에선 이슈가 생겼을 때 누구에게 연락해야 하는지 모르기에 조직도의 가장 아래에 있거나 일면식이 있는 팀원에게 연락하기 마련. 이로 인해 특정 구성원에게 자잘한 업무가 몰리고 커뮤니케이션의 비효율을 초래하기 시작하였다.\n다행히도 Jira를 도입한 상태였기에 Jira Service Management를 시작하면 되겠다고 생각, 팀의 Admin을 자처하여 구성하겠노라 선언했다. 글로벌 솔루션이니 쉬운 줄 알았거든. 과거형으로 표현한 것은 당연히 녹록지 않았다는 것. 좋은 말로는 세세한 커스터마이징이 가능한, 다른 말로는 작은 것 하나하나 다 신경 써야 하는 이 녀석을 다루며 당황했던 순간을 까먹기 전에 써두려 한다.\nJira Workflows Jira Service management를 비롯한 Jira 이슈들은 상태 값을 가지고 있다. In progress, Done 등 특정 업무의 흐름을 표시해준다. 이러한 흐름을 Workflows라는 기능으로 관리하고 있다.\n        workflows 예시    Jira도 당연히 기본적인 workflow를 제공하고 있다. 하지만, 다양한 환경에 따라 절차는 달라지기 마련. 어떤 업무는 부서장 승인을 득한 뒤 진행해야 하고, 고객과 관련된 것이라면 부서장 승인에 더해 개인정보 관리자의 승인까지 필요할 수가 있다. 이러한 요구에 따라 새롭게 만들어져야 하기에 결국 커스터마이징을 하게 된다.\n다행히 workflow를 그리는 것 자체는 어렵지 않다. 어쩌면 재밌기도 한 마지막 단계. 여차저차 필요한 workflow를 만든 후 프로젝트에 적용했는데 원하는 동작이 일어나지 않았다.\n요청이 접수되면 승인권자에게 알리기 이미 요청 양식은 준비된 상태였다. 일부 티켓은 접수 단계에서 팀장 등 상급자에게 승인을 받도록 하였고, workflow에도 관련 단계를 구성했다. 하지만 요청을 제출해도 등록한 상급자에게 알림이 가지 않았다.\n        상급자를 지정하게 되어있는 양식    양식에 옵션으로 제공되고 있지만, 이것조차 따로 설정을 해줘야 한다고. workflow에서 wating for approval을 선택하면 Add approval 옵션이 따로 있는 것을 확인할 수 있다. 추가로 Configure를 확인하여 각 환경에 맞는 구성을 해줘야 알림이 정상적으로 전달된다. 이때부터 기대하는 행동 하나하나마다 관련 옵션을 확인하기 시작했다.\n        workflow에서 확인해야 하는 옵션    지정된 승인자 구성하기 좌초된 배를 겨우 인양했나 싶었지만 바로 암초에 부딪혔다. 이번에는 미리 승인자를 지정해두는 작업. 예를 들어 고객과 관련된 요청이면 개인정보보호책임자의 승인이 필요할 것이다. 요청을 처리하는 작업자와 책임자가 동일인이라면 문제가 없겠지만, 분리되어 있기에 매번 별도의 승인이 필요할 것이다. 책임자는 정해져 있기에 매번 신청할 때마다 새로이 정하도록 하는 것은 비효율적. workflow에 미리 지정해두고자 했다.\n        선장님, 저와 같은 심정이셨나요    이러한 특수한 경우를 위해 추가로 workflow를 만들었다. 앞서 언급한 상급자의 승인을 득한 후, 특정 책임자의 추가 승인을 득하는 절차로 구성하였다. 구현하고자 하는 동작은 상급자가 승인한 경우, 책임자에게 자동으로 승인요청이 가도록 하는 것. 첫 번째 승인과 두 번째 승인을 이어주는 Transition을 선택하여 구성할 수 있다.\n        화살표(transition)을 선택하여 옵션 확인    메뉴에서 Post Functions를 선택하면 앞선 절차가 완료되면서 진행될 작업을 정의할 수 있다. 여기서 추가해야 할 것은 추가 승인자를 등록해주는 것. Add post function을 선택하면 작업을 추가할 수 있다. 주어지는 다양한 옵션 중 선택한 것은 Update Issue Custom Field인데, Custom Field로 추가해둔 권한 승인자를 이슈에 업데이트해야 승인 절차가 정상적으로 진행될 수 있기 때문이다. Custom Field의 형태가 User Picker 였기에 사용자를 지정하도록 구성할 수 있는데, 이때 각 workflow에서 필요한 사용자를 지정하면 된다.\n        권한 승인자를 미리 지정하도록    2차 승인자를 지정했다고 끝이 아니다. 연관된 Status에서 Add approval 옵션이 활성화되어야 승인 절차가 정상적으로 진행된다.\n번외, 승인 메일 양식을 바꿔주세요 승인과 얽힌 복잡한 workflow가 잘 동작하는 것을 보며 안도의 숨을 내쉬는 것도 잠시. 승인자에게 발송되는 메일 양식을 변경해달라는 요청이 발생하였다. 메일 본문에 승인/거절 버튼이 바로 보여 자세히 살펴보지 않고 관성적으로 승인할 수 있다는 의견이 제기된 것. 바로 workflow로 이동해서 양식에 대한 내용을 찾아봤지만 찾을 수 없었다. 이 옵션은 어디에 있을까?\n        거기가 어디지?    해답은 Project Settings의 Customer notification 항목에 있었다. 다양한 항목 중 Approval required로 이동하면 승인자에게 어떤 양식으로 알림을 줄지 구성할 수 있게 준비되어 있다. 그중 승인 버튼을 숨겨 요청을 직접 확인하게 하는 옵션을 선택하면 끝. 이후 발송되는 메일에는 승인 버튼이 사라지고 이슈를 확인하게끔 강제하게 된다.\n        알림 양식을 세세히 조절 가능    여기까지가 승인과 엮인 작업 마무리. 다행히도 어려움에 부딪혔을 때 사내의 다른 Jira Admin의 도움을 받아 빠르게 어려움을 해결할 수 있었다. 이번 작업을 진행하며 세세한 설정과 여기저기 숨어있는 메뉴에 감탄하게 되었음. 지금은 Automation 관련하여 시행착오를 반복하고 있는데, 이것도 마무리되면 정리할 예정이다.\n","permalink":"https://thehong.xyz/post/jira/jira-workflow/","summary":"몸담은 팀의 업무 범위가 늘어나며 문의가 점점 많아지기 시작했다. 다른 팀에선 이슈가 생겼을 때 누구에게 연락해야 하는지 모르기에 조직도의 가장 아래에 있거나 일면식이 있는 팀원에게 연락하기 마련. 이로 인해 특정 구성원에게 자잘한 업무가 몰리고 커뮤니케이션의 비효율을 초래하기 시작하였다.\n다행히도 Jira를 도입한 상태였기에 Jira Service Management를 시작하면 되겠다고 생각, 팀의 Admin을 자처하여 구성하겠노라 선언했다. 글로벌 솔루션이니 쉬운 줄 알았거든. 과거형으로 표현한 것은 당연히 녹록지 않았다는 것. 좋은 말로는 세세한 커스터마이징이 가능한, 다른 말로는 작은 것 하나하나 다 신경 써야 하는 이 녀석을 다루며 당황했던 순간을 까먹기 전에 써두려 한다.","title":"Jira Service Management 구성기 - Workflows와 Approval"},{"content":"이전에 hexo를 활용하여 블로그를 운영하다가 이런저런 핑계로 방치하고 오랜 시간이 지났다. 오랜만에 다시 글을 써보려고 했지만, 사용법을 까먹으니 쉽지 않더라. 이럴 바에 새로 시작하자 마음먹고 찾아보니 최근에는 hugo를 많이 쓴다고. 이런 트렌드엔 올라타는 것이 인지상정이라 바로 시작해본다.\n그리고 같은 실수를 반복하지 않기 위해, Windows에서 Hugo를 설정하는 여정을 까먹기 전에 써둔다.\n1. Scoop 설치 Hugo가 binary 형태 파일을 제공하기는 하지만, 아무래도 활용하기 불편하여 Scoop을 활용한 설치를 진행했다.\n많이 활용하고 있는 Ubuntu나 Mac 환경에서 활용하는 apt나 brew 같은 역할을 한다고 하는데, 이번에 처음 활용해보게 되었음.\nScoop 설치에는 PowerShell을 활용한다.\nInvoke-Expression (New-Object System.Net.WebClient).DownloadString(\u0026#39;https://get.scoop.sh\u0026#39;) 설치는 순식간에 완료되었음.\n2. Hugo 설치 Scoop을 활용하여 Hugo를 설치한다. Powershell에서 실행함.\nscoop install hugo 3. 페이지 생성 이 글을 작성하는 시점 이전에 이미 Linux 환경에서 Hugo 초기 페이지를 생성해두었기에 블로그 작업을 바로 진행할 수 있었다.\n사이트 생성부터 진행해야 하는 분이 계신다면 Hugo Quick Start 가이드를 참고하시길.\n참고로 가이드에 표시된 toml 양식은 낯설었기에 익숙한 yaml을 config 양식으로 구성하였는데, 생성하는 과정에서 확장자만 바꿔주면 간단히 적용할 수 있다.\n아래는 PaperMod 테마를 적용하여 yml 확장자의 config를 생성하는 예시이다.\necho theme = \\\u0026#34;PaperMod\\\u0026#34; \u0026gt;\u0026gt; config.yml OneDrive와 연동하여 멀티플랫폼에서 글을 작성하기 위해 우선 Windows를 선택했지만, 추후 Cloud 서비스를 이용해 Ubuntu에도 구성해볼 예정.\n","permalink":"https://thehong.xyz/post/getting-start-hugo/","summary":"이전에 hexo를 활용하여 블로그를 운영하다가 이런저런 핑계로 방치하고 오랜 시간이 지났다. 오랜만에 다시 글을 써보려고 했지만, 사용법을 까먹으니 쉽지 않더라. 이럴 바에 새로 시작하자 마음먹고 찾아보니 최근에는 hugo를 많이 쓴다고. 이런 트렌드엔 올라타는 것이 인지상정이라 바로 시작해본다.\n그리고 같은 실수를 반복하지 않기 위해, Windows에서 Hugo를 설정하는 여정을 까먹기 전에 써둔다.\n1. Scoop 설치 Hugo가 binary 형태 파일을 제공하기는 하지만, 아무래도 활용하기 불편하여 Scoop을 활용한 설치를 진행했다.\n많이 활용하고 있는 Ubuntu나 Mac 환경에서 활용하는 apt나 brew 같은 역할을 한다고 하는데, 이번에 처음 활용해보게 되었음.","title":"Getting start Hugo w/ Windows"},{"content":"주로 주피터 노트북에서 파이썬을 다루고 있는데, print를 따로 해주지 않으면 한 셀마다 마지막 결괏값만 출력되게 된다.\n1+2 2*3 2**10 위의 값을 넣고 해당 셀을 실행시키면, 마지막 값인 $2^{10}$ 의 결과인 1024만 출력될 것이다. 하지만 위와 같은 간단한 수식이나 코드를 확인하고자 매번 새로운 셀을 만들어주는 것은 여간 귀찮은 것이 아니다. 이럴 경우 다음 코드를 실행해주면, 한 셀에 입력된 수식과 함수들의 결과가 모두 출력되도록 할 수 있다.\nfrom IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026#34;all\u0026#34; 이제 다시 수식 셀을 실행하면, 각각의 결괏값이 출력된다. 다음 스크린샷을 참고. 주피터 노트북에서 한 셀에 입력된 모든 결과 출력이 가능하다.\n","permalink":"https://thehong.xyz/post/python/interactiveshell/","summary":"주로 주피터 노트북에서 파이썬을 다루고 있는데, print를 따로 해주지 않으면 한 셀마다 마지막 결괏값만 출력되게 된다.\n1+2 2*3 2**10 위의 값을 넣고 해당 셀을 실행시키면, 마지막 값인 $2^{10}$ 의 결과인 1024만 출력될 것이다. 하지만 위와 같은 간단한 수식이나 코드를 확인하고자 매번 새로운 셀을 만들어주는 것은 여간 귀찮은 것이 아니다. 이럴 경우 다음 코드를 실행해주면, 한 셀에 입력된 수식과 함수들의 결과가 모두 출력되도록 할 수 있다.\nfrom IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026#34;all\u0026#34; 이제 다시 수식 셀을 실행하면, 각각의 결괏값이 출력된다.","title":"[파이썬] 주피터 노트북, 셀 결과 모두 출력하기"},{"content":"최근에 자연어 처리 프로젝트를 진행할 기회가 있었는데, 그 시작이 네이버 카페 게시글을 크롤링하는 것이었다. 크롤링이야 쉽게 끝날 것으로 생각했었는데 녹록지 않았음. 에러도 없고 몇 번이고 경로를 확인했지만 크롤링이 되질 않았었다. 결국, 네이버 카페가 iframe 구조로 되어있기 때문에 다른 접근을 해야 한다는 것을 알게 되었다.\nOSX, Anaconda Python 3.6에 selenium, BeautifulSoup을 사용했다.\n기본적인 코드는 아래와 같음.\nfrom selenium import webdriver driver = webdriver.PhantomJS() driver.get(#cafe_url) driver.switch_to_frame(\u0026#39;cafe_main\u0026#39;) page_source = driver.page_source print(page_source) driver.close() switch_to_frame을 해줘야 원하는 작동을 하는 것을 확인할 수 있었다.\n나머지는 다른 크롤링과 동일하게 작성하면 됨.\n","permalink":"https://thehong.xyz/post/python/naver-crawl/","summary":"최근에 자연어 처리 프로젝트를 진행할 기회가 있었는데, 그 시작이 네이버 카페 게시글을 크롤링하는 것이었다. 크롤링이야 쉽게 끝날 것으로 생각했었는데 녹록지 않았음. 에러도 없고 몇 번이고 경로를 확인했지만 크롤링이 되질 않았었다. 결국, 네이버 카페가 iframe 구조로 되어있기 때문에 다른 접근을 해야 한다는 것을 알게 되었다.\nOSX, Anaconda Python 3.6에 selenium, BeautifulSoup을 사용했다.\n기본적인 코드는 아래와 같음.\nfrom selenium import webdriver driver = webdriver.PhantomJS() driver.get(#cafe_url) driver.switch_to_frame(\u0026#39;cafe_main\u0026#39;) page_source = driver.page_source print(page_source) driver.close() switch_to_frame을 해줘야 원하는 작동을 하는 것을 확인할 수 있었다.","title":"[파이썬] 네이버 카페 크롤링"},{"content":"서포트 벡터 머신(SVM)은 데이터를 나누는 최적의 초평면(Hyperplane)을 찾는 모델이다.\n위의 그림을 보면 $H_1$과 $H_2$가 데이터를 잘 분류하고 있음을 볼 수 있다. 이처럼 데이터를 나눌 수 있는 선이 많을 때는 어떻게 고를까? SVM은 $H_2$를 선택하게 되는데, $H_1$보다 더 큰 마진(Margin)을 가지고 있기 때문이다.\n데이터를 나누는 초평면이 그어지면, 해당 초평면과 가장 가까운 데이터 포인트가 있을 것이다. 그 데이터 포인트와의 거리가 마진인데, 그 마진이 클 수록 데이터를 잘 나누는 초평면으로 판단하는 것이다.\n하지만, 위의 그림처럼 초평면으로 완전히 분리되지 않는 경우는 어떻게 적용할 수 있을까? 이 경우 마진에 융통성을 부여한 Soft margin을 적용한다.\nSoft margin은 데이터 포인트가 마진이나 초평면을 넘어가는 것을 넘어가도 허용하는 것이다. 그 허용의 정도는 C라는 hyperparameter로 조절해준다. C가 클수록 허용하는 범위가 커지고, 작을수록 범위가 좁아진다.\n위의 그림에서 갈색 실선이 초평면이고, 점선이 Soft margin이다. C값에 따라 실선의 위치가 달라지고, 그에 따라 Hyperplane의 위치와 기울기도 함께 변화하게 된다. 그렇지만, Soft margin으로도 해결할 수 없는 경우가 있다.\n이와 같은 경우에는 하나의 초평면으로 분류를 할 수 없다. 그럼 어떻게 해결할 수 있을까? 우선 다항변수(Polynomial feature)를 활용하는 방법이 있다. 기존 변수에 제곱이나 세제곱 등을 적용하여 새로운 변수로 활용하는 것이다. 이를 적용하면 다음과 같은 초평면을 얻을 수 있다.\n하지만, 이 경우에는 연산량이 막대하게 증가하게 된다. 변수마다 새로운 연산이 필요하기 때문이다. 이를 극복하기 위한 것이 커널(kernel) 트릭이다. 커널은 데이터 포인트에 차원을 더해주어 공간을 넓혀준다. 차원이 증가하면서 발생한 공간에 새로운 초평면이 들어설 수 있게 되는 것이다. 다음 그림을 보면 이해하기 쉬울 것이다.\n커널과 다항변수, 둘 다 차원을 더해주는 것 같은데 왜 차이가 발생할까?\n$x = (1, 2, 3)$, $y = (4, 5, 6)$ 이라는 데이터가 주어졌을 때, 두 방식의 차이는 다음과 같다.\n   다항변수     $f(x) = (x_1x_1, x_1x_2, x_1x_3, x_2x_1, x_2x_2, x_2x_3, x_3x_1, x_3x_2, x_3x_3)$$f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)$$f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)$$16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324= 1024$       커널     $K(x,y) = (x,y)^2$$K(x,y) = (4 + 10 + 18)^2 = 1024$    두 식 모두 결과는 같지만, 그 연산과정에서 큰 차이가 발생한다. 커널 트릭을 활용함으로써 고차원에서 빠른 계산이 가능해진 것이다. 차원을 줄이는 PCA와 반대로 작용한다는 점에서 흥미로운 방법이다.\n","permalink":"https://thehong.xyz/post/analytics/svm/","summary":"서포트 벡터 머신(SVM)은 데이터를 나누는 최적의 초평면(Hyperplane)을 찾는 모델이다.\n위의 그림을 보면 $H_1$과 $H_2$가 데이터를 잘 분류하고 있음을 볼 수 있다. 이처럼 데이터를 나눌 수 있는 선이 많을 때는 어떻게 고를까? SVM은 $H_2$를 선택하게 되는데, $H_1$보다 더 큰 마진(Margin)을 가지고 있기 때문이다.\n데이터를 나누는 초평면이 그어지면, 해당 초평면과 가장 가까운 데이터 포인트가 있을 것이다. 그 데이터 포인트와의 거리가 마진인데, 그 마진이 클 수록 데이터를 잘 나누는 초평면으로 판단하는 것이다.\n하지만, 위의 그림처럼 초평면으로 완전히 분리되지 않는 경우는 어떻게 적용할 수 있을까?","title":"Support Vector Machine (SVM)"},{"content":"딥러닝은 학습시키는 시간이 길다보니, 중간중간 저장과 불러오기가 필수.\n이번에는 tflearn을 활용하는 방법을 작성하고자 한다.\n모델을 만들 때,\nmodel = tflearn.DNN(network, checkpoint_path=\u0026quot;파일명\u0026quot;)\n위와 같이 checkpoint_path=를 설정해주면, 코드가 위치한 경로에 지정한 파일명으로 checkpoint가 저장된다.\n해당 모델을 다음과 같이 실행하면 한 epoch가 완료될 때마다 파일명.setps로 checkpoint가 생성된다.\nmodel.fit(X, Y, batch_size=100, n_epoch=10, run_id='모델명', show_metric=True)\n예를 들어 10,000개의 데이터를 batch_size 100으로 fit 하게 되면 100 step마다 1 epoch가 끝나게 되고,\n파일명.100 (1 epoch)\n파일명.200 (2 epoch)\n파일명.300 (3 epoch) \u0026hellip;\n과 같이 개별 step이 표시되어 저장된다.\n계획한 모든 학습이 완료되었으면 save 명령어를 사용하여 저장할 수 있다.\nmodel.save(\u0026quot;파일명\u0026quot;)\n이렇게 저장된 checkpoint나 model을 불러오기 위해서는 load 명령어를 사용하면 되는데,\n그 때에는 model을 다시 지정해야 한다. 딥러닝에서 train과 test를 따로 진행해야 하는 주된 이유.\nmodel = tflearn.DNN(network)\nmodel.load(\u0026quot;파일명\u0026quot;)\n이렇게 로드를 하고 추가적으로 training을 하던지 predict를 진행하면 된다.\nmodel.predict(\u0026quot;파일명\u0026quot;)\n","permalink":"https://thehong.xyz/post/python/tensorflow-model-saveload/","summary":"딥러닝은 학습시키는 시간이 길다보니, 중간중간 저장과 불러오기가 필수.\n이번에는 tflearn을 활용하는 방법을 작성하고자 한다.\n모델을 만들 때,\nmodel = tflearn.DNN(network, checkpoint_path=\u0026quot;파일명\u0026quot;)\n위와 같이 checkpoint_path=를 설정해주면, 코드가 위치한 경로에 지정한 파일명으로 checkpoint가 저장된다.\n해당 모델을 다음과 같이 실행하면 한 epoch가 완료될 때마다 파일명.setps로 checkpoint가 생성된다.\nmodel.fit(X, Y, batch_size=100, n_epoch=10, run_id='모델명', show_metric=True)\n예를 들어 10,000개의 데이터를 batch_size 100으로 fit 하게 되면 100 step마다 1 epoch가 끝나게 되고,\n파일명.100 (1 epoch)\n파일명.200 (2 epoch)","title":"Tensorflow model 저장 및 불러오기"}]