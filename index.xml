<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Meet the Hong</title>
    <link>https://thehong.xyz/</link>
    <description>Recent content on Meet the Hong</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 10 Aug 2021 18:34:54 +0900</lastBuildDate><atom:link href="https://thehong.xyz/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Personal Gateway 없이 Power BI Service에서 Excel을 소스로 쓰기</title>
      <link>https://thehong.xyz/post/powerbi/source-from-web/</link>
      <pubDate>Tue, 10 Aug 2021 18:34:54 +0900</pubDate>
      
      <guid>https://thehong.xyz/post/powerbi/source-from-web/</guid>
      <description>Power BI 작업을 하면 대부분 분석계 데이터베이스를 소스로 데이터를 확보하는 경우가 대부분이다. 하지만, DB에 익숙하지 않은 조직원들이 Power BI를 활용할 때는 아무래도 Excel을 소스로 활용하게 되는 경우가 많다. 아직 DB에 쌓이지 않은 데이터를 활용해야 할 때도 마찬가지.
당연히 Excel로 소스로 활용할 수 있어 당장 개인이 작업할 때는 문제가 없다. 이슈가 생기기 시작할 때는 Power BI Service에 게시한 이후. 개인 로컬 환경에 있는 Excel을 원본으로 하는 보고서를 그대로 게시하면 이후 새로 고침을 위해 Personal Gateway 구성이 필요하다.</description>
    </item>
    
    <item>
      <title>Power BI 테넌트 서울 리전으로 변경</title>
      <link>https://thehong.xyz/post/powerbi/change-tenant/</link>
      <pubDate>Wed, 04 Aug 2021 22:34:54 +0900</pubDate>
      
      <guid>https://thehong.xyz/post/powerbi/change-tenant/</guid>
      <description>Microsoft Power BI의 구축과 운영을 담당하며 겪었던 이벤트를 기록하려 한다. 그 첫 번째로 Power BI 테넌트를 홍콩에서 서울로 이전한 경험을 까먹기 전에 공유.
Power BI 테넌트, 홍콩에 있다구? 현 조직의 Power BI를 구축한 후, Power BI Service에서 보고서를 조회할 때 뭔가 반응이 느리다는 느낌을 받아왔다. 하여 성능개선을 위해 다양한 방안을 시도하던 중, 홍콩 리전의 Power BI 테넌트와 연결된 것을 확인할 수 있었다.
        테넌트 확인 절차    테넌트를 서울로 옮기면 물리적인 거리가 줄기에 통신 시간이 줄어들 뿐더러, 해외 데이터 센터에 데이터가 저장됨으로 발생할 수 있는 각종 문제도 예방할 수 있을 것으로 기대됐다.</description>
    </item>
    
    <item>
      <title>Jira Service Management 구성기 - Workflows와 Approval</title>
      <link>https://thehong.xyz/post/jira/jira-workflow/</link>
      <pubDate>Wed, 28 Jul 2021 18:34:54 +0900</pubDate>
      
      <guid>https://thehong.xyz/post/jira/jira-workflow/</guid>
      <description>몸담은 팀의 업무 범위가 늘어나며 문의가 점점 많아지기 시작했다. 다른 팀에선 이슈가 생겼을 때 누구에게 연락해야 하는지 모르기에 조직도의 가장 아래에 있거나 일면식이 있는 팀원에게 연락하기 마련. 이로 인해 특정 구성원에게 자잘한 업무가 몰리고 커뮤니케이션의 비효율을 초래하기 시작하였다.
다행히도 Jira를 도입한 상태였기에 Jira Service Management를 시작하면 되겠다고 생각, 팀의 Admin을 자처하여 구성하겠노라 선언했다. 글로벌 솔루션이니 쉬운 줄 알았거든. 과거형으로 표현한 것은 당연히 녹록지 않았다는 것. 좋은 말로는 세세한 커스터마이징이 가능한, 다른 말로는 작은 것 하나하나 다 신경 써야 하는 이 녀석을 다루며 당황했던 순간을 까먹기 전에 써두려 한다.</description>
    </item>
    
    <item>
      <title>Getting start Hugo w/ Windows</title>
      <link>https://thehong.xyz/post/getting-start-hugo/</link>
      <pubDate>Tue, 27 Jul 2021 17:20:44 +0900</pubDate>
      
      <guid>https://thehong.xyz/post/getting-start-hugo/</guid>
      <description>이전에 hexo를 활용하여 블로그를 운영하다가 이런저런 핑계로 방치하고 오랜 시간이 지났다. 오랜만에 다시 글을 써보려고 했지만, 사용법을 까먹으니 쉽지 않더라. 이럴 바에 새로 시작하자 마음먹고 찾아보니 최근에는 hugo를 많이 쓴다고. 이런 트렌드엔 올라타는 것이 인지상정이라 바로 시작해본다.
그리고 같은 실수를 반복하지 않기 위해, Windows에서 Hugo를 설정하는 여정을 까먹기 전에 써둔다.
1. Scoop 설치 Hugo가 binary 형태 파일을 제공하기는 하지만, 아무래도 활용하기 불편하여 Scoop을 활용한 설치를 진행했다.
많이 활용하고 있는 Ubuntu나 Mac 환경에서 활용하는 apt나 brew 같은 역할을 한다고 하는데, 이번에 처음 활용해보게 되었음.</description>
    </item>
    
    <item>
      <title>[파이썬] 주피터 노트북, 셀 결과 모두 출력하기</title>
      <link>https://thehong.xyz/post/python/interactiveshell/</link>
      <pubDate>Tue, 24 Oct 2017 17:22:12 +0000</pubDate>
      
      <guid>https://thehong.xyz/post/python/interactiveshell/</guid>
      <description>주로 주피터 노트북에서 파이썬을 다루고 있는데, print를 따로 해주지 않으면 한 셀마다 마지막 결괏값만 출력되게 된다.
1+2 2*3 2**10 위의 값을 넣고 해당 셀을 실행시키면, 마지막 값인 $2^{10}$ 의 결과인 1024만 출력될 것이다. 하지만 위와 같은 간단한 수식이나 코드를 확인하고자 매번 새로운 셀을 만들어주는 것은 여간 귀찮은 것이 아니다. 이럴 경우 다음 코드를 실행해주면, 한 셀에 입력된 수식과 함수들의 결과가 모두 출력되도록 할 수 있다.
from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &amp;#34;all&amp;#34; 이제 다시 수식 셀을 실행하면, 각각의 결괏값이 출력된다.</description>
    </item>
    
    <item>
      <title>[파이썬] 네이버 카페 크롤링</title>
      <link>https://thehong.xyz/post/python/naver-crawl/</link>
      <pubDate>Sun, 08 Oct 2017 23:04:25 +0000</pubDate>
      
      <guid>https://thehong.xyz/post/python/naver-crawl/</guid>
      <description>최근에 자연어 처리 프로젝트를 진행할 기회가 있었는데, 그 시작이 네이버 카페 게시글을 크롤링하는 것이었다. 크롤링이야 쉽게 끝날 것으로 생각했었는데 녹록지 않았음. 에러도 없고 몇 번이고 경로를 확인했지만 크롤링이 되질 않았었다. 결국, 네이버 카페가 iframe 구조로 되어있기 때문에 다른 접근을 해야 한다는 것을 알게 되었다.
OSX, Anaconda Python 3.6에 selenium, BeautifulSoup을 사용했다.
기본적인 코드는 아래와 같음.
from selenium import webdriver driver = webdriver.PhantomJS() driver.get(#cafe_url) driver.switch_to_frame(&amp;#39;cafe_main&amp;#39;) page_source = driver.page_source print(page_source) driver.close() switch_to_frame을 해줘야 원하는 작동을 하는 것을 확인할 수 있었다.</description>
    </item>
    
    <item>
      <title>Support Vector Machine (SVM)</title>
      <link>https://thehong.xyz/post/analytics/svm/</link>
      <pubDate>Sun, 24 Sep 2017 19:19:28 +0000</pubDate>
      
      <guid>https://thehong.xyz/post/analytics/svm/</guid>
      <description>서포트 벡터 머신(SVM)은 데이터를 나누는 최적의 초평면(Hyperplane)을 찾는 모델이다.
위의 그림을 보면 $H_1$과 $H_2$가 데이터를 잘 분류하고 있음을 볼 수 있다. 이처럼 데이터를 나눌 수 있는 선이 많을 때는 어떻게 고를까? SVM은 $H_2$를 선택하게 되는데, $H_1$보다 더 큰 마진(Margin)을 가지고 있기 때문이다.
데이터를 나누는 초평면이 그어지면, 해당 초평면과 가장 가까운 데이터 포인트가 있을 것이다. 그 데이터 포인트와의 거리가 마진인데, 그 마진이 클 수록 데이터를 잘 나누는 초평면으로 판단하는 것이다.
하지만, 위의 그림처럼 초평면으로 완전히 분리되지 않는 경우는 어떻게 적용할 수 있을까?</description>
    </item>
    
    <item>
      <title>Tensorflow model 저장 및 불러오기</title>
      <link>https://thehong.xyz/post/python/tensorflow-model-saveload/</link>
      <pubDate>Tue, 22 Aug 2017 17:27:59 +0000</pubDate>
      
      <guid>https://thehong.xyz/post/python/tensorflow-model-saveload/</guid>
      <description>딥러닝은 학습시키는 시간이 길다보니, 중간중간 저장과 불러오기가 필수. 이번에는 tflearn을 활용하는 방법을 작성하고자 한다.
모델을 만들 때, model = tflearn.DNN(network, checkpoint_path=&amp;quot;파일명&amp;quot;) 위와 같이 checkpoint_path=를 설정해주면, 코드가 위치한 경로에 지정한 파일명으로 checkpoint가 저장된다.
해당 모델을 다음과 같이 실행하면 한 epoch가 완료될 때마다 파일명.setps로 checkpoint가 생성된다. model.fit(X, Y, batch_size=100, n_epoch=10, run_id=&#39;모델명&#39;, show_metric=True)
예를 들어 10,000개의 데이터를 batch_size 100으로 fit 하게 되면 100 step마다 1 epoch가 끝나게 되고, 파일명.100 (1 epoch) 파일명.200 (2 epoch) 파일명.</description>
    </item>
    
    
  </channel>
</rss>
